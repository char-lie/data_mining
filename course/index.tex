\input{../common/head.tex}

\begin{document}

\import{1_title/}{title.tex}

\clearpage
\setcounter{page}{2}

%\pagestyle{empty}
\tableofcontents
\thispagestyle{empty}

\clearpage
\pagestyle{fancy}

\clearpage

\chapter{Закон Ципфа}

\section{Закон Ципфа}
Отношение ранга слова $R$, то есть его номер в списке слов,
отсортированных по частоте в порядке убывания, к частоте слова $f$,
является постоянным
\begin{equation*}
  Z = R \cdot f,
\end{equation*}
где $f$ --- частота слова в тексте, а $Z$ --- коэффициент Ципфа.
Значит,
\begin{equation*}
  f = \frac{Z}{R}.
\end{equation*}

\section{Задание}

Под понятием ``отфильтровать текст'' тут и далее будут подразумеваться
следующие действия:
\begin{enumerate}
  \item
    очистить текст от всех символов кроме букв и пробелов;
  \item
    буквы привести в нижний регистр, между словами оставить по одному пробелу.
  \item
\end{enumerate}

В лабораторной работе нужно
\begin{enumerate}
  \item
    взять текст (желательно на русском языке)
    длиной более нескольких сотен килобайт;
  \item
    отфильтровать текст;
  \item
    составить частотный словарь слов --- каждому слову текста
    сопоставить количество его повторений в тексте;
  \item
    отсортировать частоты в порядке убывания;
  \item
    изобразить полученные значения на графике,
    выбрав логарифмический масштаб для оси ординат и абсцисс;
  \item
    построить степенную линию тренда и убедиться,
    что график похож на прямую линию, за исключением, возможно,
    ``хвостов'' с обеих концов.
\end{enumerate}

\section{Фильтр}
На Perl написан фильтр, который
\begin{enumerate}
  \item делает заглавные буквы строчными;
  \item убирает всё кроме пробелов, символов табуляций, переносов строк и т.п.;
  \item превращает все символы, которе не являются буквами, в пробел, также
    предотвращает появление двух пробелов подряд.
\end{enumerate}

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=perl,caption=filter.pl,
                 numbers=left]{../lab1/filter.pl}

\section{Частотный словарь}
На Python написан скрипт, который составляет частотный словарь
и выводит его в формате csv.
Полученный результат можно открыть в программе для работы
с электронными таблицами для построения графиков.

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=python,caption=counter.py,
                 numbers=left]{../lab1/counter.py}

\section{График}
\begin{figure}[h]
  \centering
  \includegraphics[width=.75\textwidth]{../lab1/chart}
  \caption{Результат}
\end{figure}

\chapter{Закон Хипса}

\section{Закон Хипса}
Объём словаря уникальных слов $\nu\left( n \right)$
для текста длиной $n$ связан с длиной текста следующим соотношением
\begin{equation*}
  \nu\left( n \right) = \alpha \cdot n^\beta,
\end{equation*}
где $\alpha$ и $\beta$ --- эмпирические константы,
которые разнятся от языка к языку,
и для европейских языков колеблятся в пределах
от $10$ до $100$ и от $0.4$ до $0.6$ соответственно.

\section{Задание}
В лабораторной работе нужно
\begin{enumerate}
  \item
    взять текст (желательно на русском языке)
    длиной более нескольких сотен килобайт;
  \item
    отфильтровать текст;
  \item
    построить зависимость количества уникальных слов в тексте
    от его размера; для этого достаточно использовать один и тот же текст,
    изымать из него всё больше и больше слов с каждой итерацией,
    и подсчитывать число уникальных слов на каждом шаге;
  \item
    изобразить полученные значения на графике;
  \item
    построить степенную линию тренда и убедиться,
    что полученные параметры $\alpha$ и $\beta$ близки к
    теоретическим значениям.
\end{enumerate}

\section{Фильтр}
На Perl написан фильтр, который
\begin{enumerate}
  \item делает заглавные буквы строчными;
  \item убирает всё кроме пробелов, символов табуляций, переносов строк и т.п.;
  \item превращает все символы, которе не являются буквами, в пробел, также
    предотвращает появление двух пробелов подряд.
\end{enumerate}

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=perl,caption=filter.pl,
                 numbers=left]{../lab2/filter.pl}

\section{Частотный словарь}
На Python написан скрипт, который считает зависимость между объёмом текста
и объёмом словаря уникальных слов и выводит его в формате csv.
Полученный результат можно открыть в программе для работы
с электронными таблицами для построения графиков.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=python,caption=counter.py,
                 numbers=left]{../lab2/count.py}

\section{График}
\begin{figure}[h]
  \centering
  \includegraphics[width=.95\textwidth]{../lab2/chart}
  \caption{Результат}
\end{figure}

\chapter{$TF-IDF$}

\section{$TF-IDF$}
Для $i$ слова ($n$-граммы) индексы $TF$ и $IDF$ считаются
по следующим формулам, где
$D$ --- множество документов,
$n_k$ --- количество повторений $k$ слова ($n$-граммы) в текущем документе
\begin{equation*}
  \begin{split}
    TF_i  &= \frac{n_i}{\sum_{k} n_k}, \\
    IDF_i &= \log {\frac{\left| D \right|}{
                   \left| \left\{ d \mid t_i \in d \in D \right\} \right|}}.
  \end{split}
\end{equation*}
Сам индекс $TF-IDF$ является произведением индексов $TF$ и $IDF$
\begin{equation*}
  TF-IDF_i = TF_i \cdot IDF_i
\end{equation*}

\section{Задание}

\subsection{Основное задание}
В лабораторной работе нужно
\begin{enumerate}
  \item
    взять текст (желательно на русском языке)
    длиной более нескольких сотен килобайт;
  \item
    отфильтровать текст;
  \item
    подсчитать $TF-IDF$ для каждого слова;
  \item
    изобразить полученные результаты в виде таблицы,
    отсортировав по значению $TF-IDF$ в порядке убывания.
\end{enumerate}

То же самое нужно проделать с биграммами и триадами слов.
Например, в тексте ``мама мыла раму'' биграммы следующие:
``мама мыла'' и ``мыла раму''.

\subsection{Стоп-слова (шумовые слова)}
Стоп-слова --- те слова, которые не несут смысловую нагрузку.
К ним относятся предлоги, частицы и прочее,
если анализируемый документ не является учебником русского языка.

Список стоп-слов можно найти в интернете.
Например, в разделе 12.9.4 Full-Text Stopwords документации к MySQL 5.5
находится список англоязычных шумовых слов.

Для увеличения скорости и уменьшения объёма обрабатываемых данных
\begin{enumerate}
  \item
    при подсчёте $TF-IDF$ для слов можно выбросить из рассмотрения те,
    которые находятся в списке стоп-слов;
    например, слово ``не'' имеет мало смысла в сказке о царе Салтане,
    чего не скажешь о слове ``лебедь'';
  \item
    при подсчёте $TF-IDF$ для биграмм следует исключать те биграммы,
    которые содержат в себе шумовые слова;
    например, биграмма ``я пришёл'' имеет мало смысловой нагрузки,
    но биграмма ``пришёл домой'' скажет больше;
  \item
    при подсчёте $TF-IDF$ для триад следует исключать те элементы,
    которые оканчиваются или начинаются на шумовые слова;
    скажем, ``и она решила'' мало о чём говорит,
    триада ``она решила пойти'' скажет больше,
    но ``решила пойти домой'' несёт определённый смысл.
\end{enumerate}

\section{Фильтр}
На Perl написан фильтр, который
\begin{enumerate}
  \item делает заглавные буквы строчными;
  \item убирает всё кроме пробелов, символов табуляций, переносов строк и т.п.;
  \item превращает все символы, которе не являются буквами, в пробел, также
    предотвращает появление двух пробелов подряд.
\end{enumerate}

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=perl,caption=filter.pl,
                 numbers=left]{../lab3/filter.pl}

\section{Счётчик $TF-IDF$}
На Python написан скрипт, который считает $TF-IDF$ для слов
и выводит их в формате csv.

Полученный результат можно открыть в программе для работы
с электронными таблицами для сортировки и фильтрации.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=python,caption=counter.py,
                 numbers=left]{../lab3/counter.py}

\section{Результат}
На рисунке \ref{fig:tfidf:words:table} изображены первые $40$ строк таблицы
со значениями $TF-IDF$ для слов из $144$ документов автора
Льва Николаевича Толстого, $27$ документов Фёдора Михайловича Достоевского
и $31$ документа Александра Сергеевича Пушкина, отсортированных по значению
$TF-IDF$ в порядке убывания.

Объём документов Толстого $18$MB, Достоевского $7.6$MB, Пушкина --- $2.8$MB.
Фильтрация происходит соответственно $10.3$, $3.2$ и $2$ секунды.
Далее каждый документ имеет только один перенос строки,
который говорит об окончании документа, и их можно объединить в один файл.
Подсчёт $TF-IDF$ происходит за $6.5$ секунд,
на выходе получается $.csv$ файл объёмом $39$MB.

%На рисунке \ref{fig:tfidf:triad:table} изображены первые $40$ строк таблицы
%с триадами.

\begin{figure}[h]
  \centering
  \includegraphics{../lab3/table}
  \caption{Результат для слов}
  \label{fig:tfidf:words:table}
\end{figure}

%\begin{figure}[h]
%  \centering
%  \includegraphics{../lab3/table}
%  \caption{Результат для триад}
%  \label{fig:tfidf:triad:table}
%\end{figure}

%\csvautotabular{lab.csv}
\csvreader[tabular=|l|l|l|c|,
  table head=\hline № & Книга & Слово & $TF-IDF$ \\\hline,
  late after line=\\\hline]
  {lab.csv}{book=\book,word=\word,tfidf=\tfidf}
  {\thecsvrow & \book & \word & \tfidf}

\end{document}

