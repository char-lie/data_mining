\input{../common/head.tex}

\begin{document}

\import{1_title/}{title.tex}

\clearpage
\setcounter{page}{2}

%\pagestyle{empty}
\tableofcontents
\thispagestyle{empty}

\clearpage
\pagestyle{fancy}

\clearpage

\chapter*{Реферат}

При интеллектуальном анализе текстовых данных важно понимать взаимосвязь между
словами внутри текста.
Также необходимо учесть, что на важность таких связей влияют ещё и связанные
документы: публикации одного автора, сборники литературы одной эпохи,
цикл учебников по определённой дисциплине и пр.

В данной работе были проверены эмпирические законы Ципфа и Хипса,
которые определяют зависимости частот слов от размера текста.
Далее была использована одна из мер значимости слов и словосочетаний в наборе
текстов --- $TF-IDF$.

\MakeUppercase{TF-IDF, закон Ципфа, закон Хипса, граф, интеллектуальный анализ}
\clearpage

\chapter*{Abstract}

Connections between words within text are needed to know, when analysing text
data.
Also it's needed to consider, that value of these connections are affected
by related documents: books of the same author, literature of one epoch,
guides for similar study etc.

Zipf's law and Heaps' law were covered by this work --- they show
correlation between words' frequencies and text size.
$TF-IDF$ was analyzed, as a measure of words value in set of texts.

\MakeUppercase{TF-IDF, закон Ципфа, закон Хипса, граф, интеллектуальный анализ}

\chapter{Закон Ципфа}

\section{Закон Ципфа}
Отношение ранга слова $R$, то есть его номер в списке слов,
отсортированных по частоте в порядке убывания, к частоте слова $f$,
является постоянным \cite{JeanBaptiste}
\begin{equation*}
  Z = R \cdot f,
\end{equation*}
где $f$ --- частота слова в тексте, а $Z$ --- коэффициент Ципфа.
Значит,
\begin{equation*}
  f = \frac{Z}{R}.
\end{equation*}

\section{Задание}

Под понятием ``отфильтровать текст'' тут и далее будут подразумеваться
следующие действия:
\begin{enumerate}
  \item
    очистить текст от всех символов кроме букв и пробелов;
  \item
    буквы привести в нижний регистр, между словами оставить по одному пробелу.
  \item
\end{enumerate}

В лабораторной работе нужно
\begin{enumerate}
  \item
    взять текст (желательно на русском языке)
    длиной более нескольких сотен килобайт;
  \item
    отфильтровать текст;
  \item
    составить частотный словарь слов --- каждому слову текста
    сопоставить количество его повторений в тексте;
  \item
    отсортировать частоты в порядке убывания;
  \item
    изобразить полученные значения на графике,
    выбрав логарифмический масштаб для оси ординат и абсцисс;
  \item
    построить степенную линию тренда и убедиться,
    что график похож на прямую линию, за исключением, возможно,
    ``хвостов'' с обеих концов.
\end{enumerate}

\section{Фильтр}
На Perl написан фильтр, который
\begin{enumerate}
  \item делает заглавные буквы строчными;
  \item убирает всё кроме пробелов, символов табуляций, переносов строк и т.п.;
  \item превращает все символы, которе не являются буквами, в пробел, также
    предотвращает появление двух пробелов подряд.
\end{enumerate}

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=perl,caption=filter.pl,
                 numbers=left]{../lab1/filter.pl}

\section{Частотный словарь}
На Python написан скрипт, который составляет частотный словарь
и выводит его в формате csv.
Полученный результат можно открыть в программе для работы
с электронными таблицами для построения графиков.

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=python,caption=counter.py,
                 numbers=left]{../lab1/counter.py}

\section{График}
\begin{figure}[h]
  \centering
  \includegraphics[width=.75\textwidth]{../lab1/chart}
  \caption{Результат}
\end{figure}

\chapter{Закон Хипса}

\section{Закон Хипса}
Объём словаря уникальных слов $\nu\left( n \right)$
для текста длиной $n$ связан с длиной текста следующим соотношением
\cite{Internetika}
\begin{equation*}
  \nu\left( n \right) = \alpha \cdot n^\beta,
\end{equation*}
где $\alpha$ и $\beta$ --- эмпирические константы,
которые разнятся от языка к языку,
и для европейских языков колеблятся в пределах
от $10$ до $100$ и от $0.4$ до $0.6$ соответственно.

\section{Задание}
В лабораторной работе нужно
\begin{enumerate}
  \item
    взять текст (желательно на русском языке)
    длиной более нескольких сотен килобайт;
  \item
    отфильтровать текст;
  \item
    построить зависимость количества уникальных слов в тексте
    от его размера; для этого достаточно использовать один и тот же текст,
    изымать из него всё больше и больше слов с каждой итерацией,
    и подсчитывать число уникальных слов на каждом шаге;
  \item
    изобразить полученные значения на графике;
  \item
    построить степенную линию тренда и убедиться,
    что полученные параметры $\alpha$ и $\beta$ близки к
    теоретическим значениям.
\end{enumerate}

\section{Фильтр}
На Perl написан фильтр, который
\begin{enumerate}
  \item делает заглавные буквы строчными;
  \item убирает всё кроме пробелов, символов табуляций, переносов строк и т.п.;
  \item превращает все символы, которе не являются буквами, в пробел, также
    предотвращает появление двух пробелов подряд.
\end{enumerate}

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=perl,caption=filter.pl,
                 numbers=left]{../lab2/filter.pl}

\section{Частотный словарь}
На Python написан скрипт, который считает зависимость между объёмом текста
и объёмом словаря уникальных слов и выводит его в формате csv.
Полученный результат можно открыть в программе для работы
с электронными таблицами для построения графиков.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=python,caption=counter.py,
                 numbers=left]{../lab2/count.py}

\section{График}
\begin{figure}[h]
  \centering
  \includegraphics[width=.95\textwidth]{../lab2/chart}
  \caption{Результат}
\end{figure}

\chapter{$TF-IDF$}

\section{$TF-IDF$}
Для $i$ слова ($n$-граммы) индексы $TF$ и $IDF$ считаются
по следующим формулам, где
$D$ --- множество документов,
$n_k$ --- количество повторений $k$ слова ($n$-граммы) в текущем документе
\cite{Jones}
\begin{equation*}
  \begin{split}
    TF_i  &= \frac{n_i}{\sum_{k} n_k}, \\
    IDF_i &= \log {\frac{\left| D \right|}{
                   \left| \left\{ d \mid t_i \in d \in D \right\} \right|}}.
  \end{split}
\end{equation*}
Сам индекс $TF-IDF$ является произведением индексов $TF$ и $IDF$
\begin{equation*}
  TF-IDF_i = TF_i \cdot IDF_i
\end{equation*}

\section{Задание}

\subsection{Основное задание}
В лабораторной работе нужно
\begin{enumerate}
  \item
    взять текст (желательно на русском языке)
    длиной более нескольких сотен килобайт;
  \item
    отфильтровать текст;
  \item
    подсчитать $TF-IDF$ для каждого слова;
  \item
    изобразить полученные результаты в виде таблицы,
    отсортировав по значению $TF-IDF$ в порядке убывания.
\end{enumerate}

То же самое нужно проделать с биграммами и триадами слов.
Например, в тексте ``мама мыла раму'' биграммы следующие:
``мама мыла'' и ``мыла раму''.

\subsection{Стоп-слова (шумовые слова)}
Стоп-слова --- те слова, которые не несут смысловую нагрузку.
К ним относятся предлоги, частицы и прочее,
если анализируемый документ не является учебником русского языка.

Список стоп-слов можно найти в интернете.
Например, в разделе 12.9.4 Full-Text Stopwords документации к MySQL 5.5
\cite{Oracle} находится список англоязычных шумовых слов.

Для увеличения скорости и уменьшения объёма обрабатываемых данных
\begin{enumerate}
  \item
    при подсчёте $TF-IDF$ для слов можно выбросить из рассмотрения те,
    которые находятся в списке стоп-слов;
    например, слово ``не'' имеет мало смысла в сказке о царе Салтане,
    чего не скажешь о слове ``лебедь'';
  \item
    при подсчёте $TF-IDF$ для биграмм следует исключать те биграммы,
    которые содержат в себе шумовые слова;
    например, биграмма ``я пришёл'' имеет мало смысловой нагрузки,
    но биграмма ``пришёл домой'' скажет больше;
  \item
    при подсчёте $TF-IDF$ для триад следует исключать те элементы,
    которые оканчиваются или начинаются на шумовые слова;
    скажем, ``и она решила'' мало о чём говорит,
    триада ``она решила пойти'' скажет больше,
    но ``решила пойти домой'' несёт определённый смысл.
\end{enumerate}

\section{Фильтр}
На Perl написан фильтр, который
\begin{enumerate}
  \item делает заглавные буквы строчными;
  \item убирает всё кроме пробелов, символов табуляций, переносов строк и т.п.;
  \item превращает все символы, которе не являются буквами, в пробел, также
    предотвращает появление двух пробелов подряд.
\end{enumerate}

Вход считывается из stdin, выход происходит в stdout.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=perl,caption=filter.pl,
                 numbers=left]{../lab3/filter.pl}

\section{Счётчик $TF-IDF$}
На Python написан скрипт, который считает $TF-IDF$ для слов
и выводит их в формате csv.

Полученный результат можно открыть в программе для работы
с электронными таблицами для сортировки и фильтрации.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=python,caption=counter.py,
                 numbers=left]{../lab3/counter.py}

\section{Результат}
На \ref{fig:tfidf:words:table} изображены первые $18$ строк таблицы
со значениями $TF-IDF$ для слов из $144$ документов автора
Льва Николаевича Толстого, $27$ документов Фёдора Михайловича Достоевского
и $31$ документа Александра Сергеевича Пушкина, отсортированных по значению
$TF-IDF$ в порядке убывания.

Объём документов Толстого $18$MB, Достоевского $7.6$MB, Пушкина --- $2.8$MB.
Фильтрация происходит соответственно $10.3$, $3.2$ и $2$ секунды.
Далее каждый документ имеет только один перенос строки,
который говорит об окончании документа, и их можно объединить в один файл.
Подсчёт $TF-IDF$ происходит за $6.5$ секунд,
на выходе получается $.csv$ файл объёмом $39$MB.

На \ref{fig:tfidf:bigrams:table} изображены первые $18$ строк таблицы
с биграммами, а на \ref{fig:tfidf:triads:table} изображены
первые $18$ строк таблицы с триадами.

\begin{table}[h]
  \centering
  \csvreader[tabular=|l|l|l|c|,
  table head=\hline № & Книга & Слово & $TF-IDF$ \\\hline,
  late after line=\\\hline]
  {lab3.1.csv}{book=\book,word=\word,tfidf=\tfidf}
  {\thecsvrow & \book & \word & \tfidf}

  \caption{Результат для слов}
  \label{fig:tfidf:words:table}
\end{table}

\begin{table}[h]
  \centering
  \csvreader[tabular=|l|l|l|c|,
  table head=\hline № & Книга & Биграмма & $TF-IDF$ \\\hline,
  late after line=\\\hline]
  {lab3.2.csv}{book=\book,word=\word,tfidf=\tfidf}
  {\thecsvrow & \book & \word & \tfidf}

  \caption{Результат для биграмм}
  \label{fig:tfidf:bigrams:table}
\end{table}

\begin{table}[h]
  \centering
  \csvreader[tabular=|l|l|l|c|,
  table head=\hline № & Книга & Триада & $TF-IDF$ \\\hline,
  late after line=\\\hline]
  {lab3.3.csv}{book=\book,word=\word,tfidf=\tfidf}
  {\thecsvrow & \book & \word & \tfidf}

  \caption{Результат для триад}
  \label{fig:tfidf:triads:table}
\end{table}

\chapter{Графическое представление сети слов}

\section{Задание}
Изобразить граф, отображающий взаимосвязи между словами, биграммами и триадами.
Привести его матрицу весов.

\section{Прорисовка графа}

Связи строились следующим образом:
от слов шли дуги к биграммам, в которые они входят,
а от биграмм --- к триадам, в которые входят эти биграммы.
Было взято $10$ самых значимых триад из произведения ``Каменный гость''
Александра Сергеевича Пушкина, биграммы, которые в них входят, и слова,
которые входят в эти биграммы и не попадают в чёрный список.
Вес дужки --- индекс $TF-IDF$ ``родителя''.
То есть, вес дужки между словом и биграммой --- $TF-IDF$ биграммы,
вес дужки между биграммой и триадой --- $TF-IDF$ триады.

Поскольку между словами и триадами дужек нет, для уменьшения объёма таблицы
с матрицей весов было решено построить две таблицы:
слова и биграммы (табл. \ref{fig:matrix:bigrams:table}),
биграммы и триады (табл. \ref{fig:matrix:triads:table}).

Для визуализации графа была использована библиотека $graph-tool$.
Прорисовка выполнялась с помощью иерархического разделения блоков \cite{Graph}.
Результат можно увидеть на рисунке \ref{fig:triads:graph}.
Чем жирнее вершина, тем больше у неё $TF-IDF$.

Ниже приведён код программы визуализации.

\lstset{inputencoding=utf8, extendedchars=\true}
\lstinputlisting[language=python,caption=draw.py,
                 numbers=left]{../lab4/main.py}

\begin{sidewaystable}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
    & анна & прощай & неизбежна & лаура & гуан & дон & несчастный & дона & бесподобно & страдальца & опасный & прощайте & нарочно \\ \hline

    несчастный дон & 0 & 0 & 0 & 0 & 0 & 0.471297 & 0.002683 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
    опасный дон & 0 & 0 & 0 & 0 & 0 & 0.471297 & 0 & 0 & 0 & 0 & 0.006097 & 0 & 0 \\ \hline
    дон гуан & 0 & 0 & 0 & 0 & 0.625634 & 0.471297 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
    бесподобно лаура & 0 & 0 & 0 & 0.192471 & 0 & 0 & 0 & 0 & 0.007633 & 0 & 0 & 0 & 0 \\ \hline
    дона анна & 0.143801 & 0 & 0 & 0 & 0 & 0.471297 & 0 & 0.222531 & 0 & 0 & 0 & 0 & 0 \\ \hline
    неизбежна дон & 0 & 0 & 0.004184 & 0 & 0 & 0.471297 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
    страдальца дона & 0 & 0 & 0 & 0 & 0 & 0.471297 & 0 & 0.222531 & 0 & 0.003048 & 0 & 0 & 0 \\ \hline
    нарочно дон & 0 & 0 & 0 & 0 & 0 & 0.471297 & 0 & 0 & 0 & 0 & 0 & 0 & 0.001116 \\ \hline
    лаура прощайте & 0 & 0.003127 & 0 & 0.192471 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.003458 & 0 \\ \hline
  \end{tabular}
}
  \caption{Матрица весов графа слов и биграмм}
  \label{fig:matrix:bigrams:table}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
    \hline
      & неизбежна дон & лаура прощайте & опасный дон & дона анна & несчастный дон & нарочно дон & страдальца дона & дон гуан & бесподобно лаура \\ \hline
      страдальца дона анна & 0 & 0 & 0 & 0.461864 & 0 & 0 & 0.006327 & 0 & 0 \\ \hline
      несчастный дон гуан & 0 & 0 & 0 & 0 & 0.006327 & 0 & 0 & 0.803516 & 0 \\ \hline
      бесподобно лаура прощайте & 0 & 0.006327 & 0 & 0 & 0 & 0 & 0 & 0 & 0.006327 \\ \hline
      нарочно дон гуан & 0 & 0 & 0 & 0 & 0 & 0.006327 & 0 & 0.803516 & 0 \\ \hline
      неизбежна дон гуан & 0.006327 & 0 & 0 & 0 & 0 & 0 & 0 & 0.803516 & 0 \\ \hline
      опасный дон гуан & 0 & 0 & 0.006327 & 0 & 0 & 0 & 0 & 0.803516 & 0 \\ \hline
  \end{tabular}
}
  \caption{Матрица весов графа биграмм и триад}
  \label{fig:matrix:triads:table}
\end{sidewaystable}

\begin{figure}[h]
  \centering
  \includegraphics[width=.75\textwidth]{triads.png}
  \caption{triads}
  \label{fig:triads:graph}
\end{figure}

\clearpage

\chapter*{Выводы}
\addcontentsline{toc}{chapter}{Выводы}

В работе была проанализирована русскоязычная классика:
Пушкин, Толстой и Достоевский.
Эмпирические законы Ципфа и Хипса подтвердились.
$TF-IDF$, хоть и не является хорошей мерой,
дал хорошие результаты.

Полученная в итоге сеть слов была изображена в виде графа, построенного
с помощью метода иерархического разделения блоков (Hierarchical Block Partition).
Метод $TF-IDF$ проявил себя как простой и рабочий,
а визуализация графов посредством библиотеки $graph-tool$ для $Python$ ---
удобная и простая в использовании утилита, позволяющая получить наглядную
картину.

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Список литературы}
\renewcommand\bibname{Список литературы}
\bibliography{../common/bibliography.bib}
\end{document}

